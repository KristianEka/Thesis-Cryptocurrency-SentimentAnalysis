{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thhABeprZU60"
   },
   "source": [
    "# **Library Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "arGsYkHVZI2L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\krist\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Library\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "H6am0RGdaUx5"
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CjzRXABZXxS"
   },
   "source": [
    "# **Read Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "OwGurEKXZZFM",
    "outputId": "3ce984e4-b6cb-4c02-808b-eaf4328beca3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>vader_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Token: $GROK24 - Grok 2024 Network: Ethereum ...</td>\n",
       "      <td>token grok grok network ethereum contract xccc...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@metaversejoji Let's check @SolanaMono $SOL #W...</td>\n",
       "      <td>let check sol</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Day's DCA: $BTC, $ATOM, $DVPN, $AXL, $JKL, $HU...</td>\n",
       "      <td>day dca btc atom dvpn axl jkl huahua</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@BorkSOL @Cerita_Crypto @solana @aeyakovenko Y...</td>\n",
       "      <td>project really amazing thats followed send please</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>üëâ WL FOR .0 SOL MINT üëà üëâ40 HOURS TILL SNAPSHOT...</td>\n",
       "      <td>sol mint hour till snapshot requirement join d...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9879</th>\n",
       "      <td>CyberKong VX #11328 was adopted for  0.18 $ETH...</td>\n",
       "      <td>cyberkong adopted eth blur</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9880</th>\n",
       "      <td>BULLISH ON SOLANA BULLISH ON JUP BULLISH ON MA...</td>\n",
       "      <td>bullish solana bullish jup bullish madlads</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9881</th>\n",
       "      <td>@naija_bitcoin üçøüçøüçøüçøüçø rd to 3k before valentine...</td>\n",
       "      <td>valentine</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9882</th>\n",
       "      <td>Binance Futures #KLAY/ #USDT Take-Profit targe...</td>\n",
       "      <td>binance future takeprofit target profit period...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>@Ta_hmid @Starknet You most likely in the luck...</td>\n",
       "      <td>likely lucky one</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9843 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              full_text  \\\n",
       "0     'Token: $GROK24 - Grok 2024 Network: Ethereum ...   \n",
       "1     @metaversejoji Let's check @SolanaMono $SOL #W...   \n",
       "2     Day's DCA: $BTC, $ATOM, $DVPN, $AXL, $JKL, $HU...   \n",
       "3     @BorkSOL @Cerita_Crypto @solana @aeyakovenko Y...   \n",
       "4     üëâ WL FOR .0 SOL MINT üëà üëâ40 HOURS TILL SNAPSHOT...   \n",
       "...                                                 ...   \n",
       "9879  CyberKong VX #11328 was adopted for  0.18 $ETH...   \n",
       "9880  BULLISH ON SOLANA BULLISH ON JUP BULLISH ON MA...   \n",
       "9881  @naija_bitcoin üçøüçøüçøüçøüçø rd to 3k before valentine...   \n",
       "9882  Binance Futures #KLAY/ #USDT Take-Profit targe...   \n",
       "9883  @Ta_hmid @Starknet You most likely in the luck...   \n",
       "\n",
       "                                         processed_text vader_sentiment  \n",
       "0     token grok grok network ethereum contract xccc...        Positive  \n",
       "1                                         let check sol         Neutral  \n",
       "2                  day dca btc atom dvpn axl jkl huahua         Neutral  \n",
       "3     project really amazing thats followed send please        Positive  \n",
       "4     sol mint hour till snapshot requirement join d...        Positive  \n",
       "...                                                 ...             ...  \n",
       "9879                         cyberkong adopted eth blur         Neutral  \n",
       "9880         bullish solana bullish jup bullish madlads         Neutral  \n",
       "9881                                          valentine         Neutral  \n",
       "9882  binance future takeprofit target profit period...        Positive  \n",
       "9883                                   likely lucky one        Positive  \n",
       "\n",
       "[9843 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_excel('XCleanCryptocurrencyDataset.xlsx', index_col=0)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iK96gIM6ZcpR"
   },
   "source": [
    "# **Data Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2qZ8YkqjZhMl"
   },
   "outputs": [],
   "source": [
    "# Asumsikan df adalah DataFrame Anda yang sudah dimuat\n",
    "X = df['processed_text']  # Kolom teks yang sudah diproses\n",
    "y = df['vader_sentiment']  # Target/Label\n",
    "\n",
    "# Encoding target\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Tokenisasi dan pembuatan sequences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(sequences, maxlen=1000)  # Sesuaikan maxlen sesuai dengan kebutuhan default:100\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Batasan feature selection\n",
    "dim = X_train.shape[1]  # Jumlah token maksimal dalam sequences\n",
    "lb = [0] * dim\n",
    "ub = [1] * dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming df is your loaded DataFrame\n",
    "# X = df['processed_text']  # Preprocessed text column\n",
    "# y = df['vader_sentiment']  # Target/Label\n",
    "\n",
    "# # Encoding the target\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "# y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# # Apply TF-IDF encoding instead of tokenization and padding\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=100)  # Adjust max_features as needed\n",
    "# X_tfidf = tfidf_vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# # Splitting data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Since TF-IDF produces a dense matrix of features, we adjust the feature selection bounds accordingly\n",
    "# dim = X_train.shape[1]  # The new dimension is the number of features from TF-IDF\n",
    "# lb = [0] * dim  # Lower bound for each feature\n",
    "# ub = [1] * dim  # Upper bound for each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iwrBTfWZgMM"
   },
   "source": [
    "# **Feature Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shMg_07NbzFn"
   },
   "source": [
    "# Particle Swarm Optimization (PSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pso(lb, ub, ieqcons=[], f_ieqcons=None, args=(), kwargs={}, swarmsize=100, omega=0.5, phip=0.5, phig=0.5, maxiter=100, minstep=1e-8, minfunc=1e-8, debug=False):\n",
    "    \"\"\"\n",
    "    Perform a particle swarm optimization (PSO) without an external objective function.\n",
    "\n",
    "    Parameters:\n",
    "    - lb: lower bounds of the design variables\n",
    "    - ub: upper bounds of the design variables\n",
    "    - ieqcons: list of inequality constraint functions (optional)\n",
    "    - f_ieqcons: function returning a list of inequality constraints (optional)\n",
    "    - args: additional arguments passed to the internal evaluation function\n",
    "    - kwargs: additional keyword arguments passed to the internal evaluation function\n",
    "    - swarmsize: number of particles in the swarm\n",
    "    - omega: particle velocity scaling factor\n",
    "    - phip: scaling factor to search away from the particle's best known position\n",
    "    - phig: scaling factor to search away from the swarm's best known position\n",
    "    - maxiter: maximum number of iterations\n",
    "    - minstep: minimum step size of swarm's best position before the search terminates\n",
    "    - minfunc: minimum change of swarm's best objective value before the search terminates\n",
    "    - debug: if True, progress statements will be displayed every iteration\n",
    "\n",
    "    Returns:\n",
    "    - g: the swarm's best known position (optimal design)\n",
    "    - f: the objective value at g\n",
    "    \"\"\"\n",
    "    dim = len(lb)\n",
    "    # Initialize the particle positions and their velocities\n",
    "    positions = np.random.uniform(low=lb, high=ub, size=(swarmsize, dim))\n",
    "    velocities = np.zeros((swarmsize, dim))\n",
    "    # Initialize the global and local best positions\n",
    "    personal_best_positions = positions.copy()\n",
    "    personal_best_values = np.array([np.inf for _ in range(swarmsize)])\n",
    "    global_best_value = np.inf\n",
    "    global_best_position = None\n",
    "\n",
    "    def evaluate(position):\n",
    "        # Define the objective directly inside PSO. For demonstration, using sum of squares.\n",
    "        return np.sum(position**2)\n",
    "\n",
    "    for iteration in range(maxiter):\n",
    "        # Update velocities and positions\n",
    "        for i in range(swarmsize):\n",
    "            r_p, r_g = np.random.rand(dim), np.random.rand(dim)\n",
    "            velocities[i] = omega * velocities[i] + \\\n",
    "                            phip * r_p * (personal_best_positions[i] - positions[i]) + \\\n",
    "                            phig * r_g * (global_best_position - positions[i]) if global_best_position is not None else 0\n",
    "            positions[i] += velocities[i]\n",
    "            positions[i] = np.clip(positions[i], lb, ub)  # Keep within bounds\n",
    "\n",
    "            # Evaluate the fitness directly without an external function\n",
    "            value = evaluate(positions[i])\n",
    "            # Update the personal best\n",
    "            if value < personal_best_values[i]:\n",
    "                personal_best_positions[i] = positions[i]\n",
    "                personal_best_values[i] = value\n",
    "            # Update the global best\n",
    "            if value < global_best_value:\n",
    "                global_best_position = positions[i]\n",
    "                global_best_value = value\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Iteration {iteration}: Best Value = {global_best_value}\")\n",
    "\n",
    "        # Check for early stopping criteria\n",
    "        if np.abs(global_best_value - personal_best_values.min()) < minfunc or np.linalg.norm(velocities.max()) < minstep:\n",
    "            break\n",
    "\n",
    "    return global_best_position, global_best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Position: [0.35991506 0.25832553 0.17457657 0.70890614 0.70901387 0.12253228\n",
      " 0.16202495 0.86759261 0.67683665 0.73486348 0.14925183 0.07399279\n",
      " 0.39945882 0.46360679 0.45862317 0.54375208 0.63640685 0.44840585\n",
      " 0.17753393 0.49907648 0.38063109 0.40731705 0.15675811 0.4992129\n",
      " 0.70485381 0.57937239 0.73671191 0.84708222 0.67493368 0.24590818\n",
      " 0.52458943 0.26237659 0.35745392 0.39749861 0.53717026 0.60887031\n",
      " 0.61984983 0.45545886 0.28010228 0.59271557 0.7315377  0.58581156\n",
      " 0.37169813 0.92119942 0.32553578 0.55686064 0.18565107 0.75142074\n",
      " 0.65531181 0.41182608 0.68810899 0.29917674 0.24166755 0.45391906\n",
      " 0.63243145 0.19378382 0.60617622 0.60868018 0.04524871 0.49343289\n",
      " 0.49698172 0.663065   0.77628783 0.29755796 0.61969487 0.21372087\n",
      " 0.41357171 0.91158501 0.78872063 0.50445342 0.23618572 0.25458398\n",
      " 0.7147852  0.52496208 0.69921876 0.71141705 0.3217466  0.2632762\n",
      " 0.05243295 0.18992464 0.31200702 0.24415049 0.42788616 0.78257428\n",
      " 0.68743916 0.51442654 0.52487002 0.69400317 0.18760384 0.48348432\n",
      " 0.53126708 0.616208   0.75639264 0.84474811 0.56863029 0.9162973\n",
      " 0.64947538 0.26479128 0.56245774 0.93188411 0.67685745 0.90371179\n",
      " 0.02907127 0.46754588 0.39311278 0.47701329 0.06238104 0.87172836\n",
      " 0.68672147 0.09728035 0.36034687 0.55164514 0.86510236 0.38328477\n",
      " 0.43580812 0.24060595 0.18229733 0.54636868 0.77058123 0.78107501\n",
      " 0.7151626  0.11553431 0.54087698 0.41632718 0.36471291 0.50158441\n",
      " 0.17721049 0.78654344 0.35808669 0.37346096 0.19644626 0.46118666\n",
      " 0.50664461 0.50005435 0.31731587 0.66780975 0.47149842 0.92133156\n",
      " 0.12380558 0.22638752 0.67612509 0.42014712 0.39692902 0.31603339\n",
      " 0.544211   0.17767297 0.6081621  0.59380884 0.3168501  0.76901513\n",
      " 0.52642233 0.71702343 0.80703953 0.62749893 0.29594749 0.17283917\n",
      " 0.12145448 0.62102048 0.19551349 0.38348301 0.77598682 0.90023941\n",
      " 0.23406399 0.14979818 0.05597713 0.37284475 0.71210399 0.18443063\n",
      " 0.57251601 0.1731466  0.25190545 0.13155122 0.18916476 0.15726372\n",
      " 0.05857955 0.80278508 0.31019965 0.32780623 0.25804855 0.64589171\n",
      " 0.30867065 0.63804043 0.35217271 0.44587891 0.44371422 0.57634542\n",
      " 0.32530483 0.40047716 0.66655835 0.42163317 0.40181211 0.56007052\n",
      " 0.95064123 0.59649838 0.57170555 0.26666332 0.57736938 0.71880727\n",
      " 0.26037268 0.77738586 0.64151761 0.26541422 0.76430098 0.08729061\n",
      " 0.86615947 0.45371945 0.77032896 0.19708804 0.54475419 0.17600128\n",
      " 0.31559743 0.64634973 0.48025505 0.61229737 0.79023555 0.46888699\n",
      " 0.47079549 0.74882924 0.21580642 0.21921227 0.70689943 0.77495437\n",
      " 0.62402229 0.62049418 0.56893152 0.56846282 0.29373939 0.59144555\n",
      " 0.47381405 0.80387009 0.10268785 0.30642949 0.09867094 0.84371757\n",
      " 0.81656136 0.77860716 0.66750291 0.13737297 0.41131818 0.69479549\n",
      " 0.51420216 0.59470232 0.25998672 0.46727721 0.38177966 0.89812651\n",
      " 0.44537838 0.2613457  0.20274853 0.2036682  0.5941668  0.72986057\n",
      " 0.69797202 0.54271301 0.58370921 0.66913006 0.88748721 0.30226111\n",
      " 0.60573882 0.87994871 0.02029302 0.27566341 0.76145766 0.34497476\n",
      " 0.44190647 0.56666124 0.42085174 0.58226711 0.28746244 0.09528589\n",
      " 0.57552271 0.52875588 0.64445431 0.0652782  0.14624197 0.19228539\n",
      " 0.45711559 0.60419969 0.35099282 0.77396679 0.24155831 0.65566047\n",
      " 0.70752846 0.80751035 0.5875643  0.062264   0.2920074  0.3148077\n",
      " 0.47607097 0.3181214  0.60929037 0.43939158 0.41119695 0.71046009\n",
      " 0.0904889  0.69483505 0.05632515 0.45572341 0.86756036 0.42044551\n",
      " 0.58224081 0.39760844 0.81287451 0.70363787 0.79467285 0.10565683\n",
      " 0.23033749 0.51923261 0.88342705 0.47601047 0.47416878 0.85021874\n",
      " 0.55387595 0.70118051 0.17938639 0.28642876 0.51342959 0.71079464\n",
      " 0.32383516 0.46505088 0.42074982 0.33563455 0.83210446 0.67065927\n",
      " 0.17971953 0.33519845 0.75378925 0.24681838 0.27086116 0.5623861\n",
      " 0.5409257  0.72722927 0.1677252  0.41708417 0.64245587 0.46562438\n",
      " 0.21154535 0.2311082  0.46497643 0.27827437 0.31384908 0.4444296\n",
      " 0.55674045 0.30525845 0.46127848 0.3798549  0.16457056 0.75129882\n",
      " 0.73062894 0.75378584 0.29970928 0.45273863 0.33484858 0.34782798\n",
      " 0.25469229 0.72198536 0.6864241  0.40812309 0.80746345 0.21015595\n",
      " 0.37054187 0.68337872 0.52678209 0.18438165 0.52693603 0.69697393\n",
      " 0.77406318 0.31084887 0.50316289 0.26519476 0.15436577 0.14273156\n",
      " 0.43445995 0.67821054 0.77296833 0.54080465 0.60690066 0.27144693\n",
      " 0.32181212 0.39019223 0.63138296 0.25025287 0.18014893 0.56133132\n",
      " 0.20470978 0.39757836 0.85205778 0.71891351 0.1058582  0.12206645\n",
      " 0.12059833 0.12835914 0.31175997 0.38280075 0.19767524 0.71901585\n",
      " 0.59119457 0.52311342 0.43961498 0.27111194 0.55804509 0.64417666\n",
      " 0.44358029 0.27737684 0.71686011 0.73285424 0.95076624 0.23411074\n",
      " 0.49395069 0.78560299 0.35999341 0.37588929 0.67875847 0.53119631\n",
      " 0.83936708 0.56517929 0.3323759  0.22705198 0.80325257 0.41755393\n",
      " 0.55148635 0.36855336 0.63834407 0.55684764 0.30179709 0.52072283\n",
      " 0.44277712 0.72048303 0.94690958 0.62685983 0.43527934 0.53090634\n",
      " 0.54552732 0.44981735 0.62550417 0.37517586 0.37711981 0.34991212\n",
      " 0.23769975 0.78599669 0.75056964 0.52870599 0.88931422 0.34411368\n",
      " 0.24157951 0.82995232 0.65882834 0.52164156 0.74045421 0.27572534\n",
      " 0.69708152 0.33652343 0.28299861 0.86418924 0.22843161 0.41600672\n",
      " 0.34788426 0.33729783 0.60048023 0.10617927 0.88427047 0.17636413\n",
      " 0.77980485 0.31214401 0.09322977 0.47545773 0.35628705 0.50473571\n",
      " 0.58807067 0.82282574 0.90169381 0.46845126 0.24857361 0.31158798\n",
      " 0.16107642 0.36443443 0.70964609 0.57839191 0.62979745 0.79053376\n",
      " 0.12843405 0.77774597 0.69820049 0.72841653 0.24133252 0.49765318\n",
      " 0.60107653 0.67101165 0.72155058 0.5451905  0.46243424 0.57154305\n",
      " 0.18519719 0.2352859  0.34239988 0.33434334 0.75958239 0.51074394\n",
      " 0.91059064 0.60907137 0.27858151 0.62865783 0.3836885  0.86275812\n",
      " 0.72334002 0.14114109 0.32382867 0.44753885 0.2774912  0.54797687\n",
      " 0.27799101 0.66392336 0.16593466 0.71987823 0.38081324 0.43096539\n",
      " 0.56097577 0.23469257 0.79704588 0.04305635 0.51913536 0.21185117\n",
      " 0.44212391 0.11540565 0.36163916 0.21525698 0.45074252 0.12282442\n",
      " 0.48980037 0.3522199  0.373391   0.35212341 0.52545871 0.75338468\n",
      " 0.51287391 0.63383681 0.63747576 0.45702575 0.10185601 0.52434973\n",
      " 0.53435807 0.31498849 0.87739416 0.57751734 0.2083651  0.50182266\n",
      " 0.483163   0.58212577 0.30953232 0.1386482  0.47549767 0.19700239\n",
      " 0.17036939 0.65708635 0.30849613 0.43425177 0.41374652 0.53825925\n",
      " 0.59575087 0.38581587 0.25867799 0.54746488 0.5880207  0.43931087\n",
      " 0.76934777 0.80200197 0.47532262 0.64645261 0.87211899 0.27169678\n",
      " 0.16271043 0.38652057 0.40128508 0.24764841 0.21281449 0.29395618\n",
      " 0.16372854 0.5220525  0.2383638  0.44536988 0.61771422 0.76696997\n",
      " 0.67632891 0.08248239 0.25305289 0.81537853 0.12286428 0.61087702\n",
      " 0.30841245 0.48391709 0.83127814 0.64494769 0.28160019 0.64182822\n",
      " 0.13700479 0.83358039 0.29491428 0.7283718  0.69322088 0.30365441\n",
      " 0.59032746 0.35729935 0.63074576 0.77977548 0.51488847 0.25488828\n",
      " 0.81643855 0.56542315 0.19781288 0.66662911 0.63551819 0.76739824\n",
      " 0.17813953 0.42873679 0.32737028 0.74531824 0.41087762 0.55757388\n",
      " 0.23987442 0.81715017 0.14163717 0.85726793 0.75737202 0.56523649\n",
      " 0.7017739  0.71376032 0.62279664 0.66851412 0.85518341 0.42472114\n",
      " 0.43523536 0.86720276 0.48642261 0.62362086 0.41002256 0.4731666\n",
      " 0.54716187 0.37414383 0.94682543 0.56983953 0.34360238 0.76957253\n",
      " 0.24957295 0.43422308 0.48121367 0.27663182 0.94870772 0.56744413\n",
      " 0.25713775 0.60533636 0.34292379 0.46626234 0.57461351 0.49492133\n",
      " 0.21747092 0.46908262 0.65028089 0.24366203 0.16766662 0.13596871\n",
      " 0.24194199 0.46772649 0.26510166 0.28558247 0.46827022 0.36871407\n",
      " 0.76353021 0.56373217 0.3737482  0.53209281 0.346228   0.46174701\n",
      " 0.29326259 0.3841233  0.26199939 0.2650216  0.45978168 0.81871863\n",
      " 0.11289374 0.10198201 0.7440772  0.16039934 0.36029718 0.50370162\n",
      " 0.77838867 0.34133365 0.77985481 0.40894688 0.71851183 0.47481911\n",
      " 0.68935559 0.21074802 0.32711674 0.30436621 0.34221004 0.55451627\n",
      " 0.59830965 0.22711108 0.76976903 0.37850961 0.52776712 0.77580113\n",
      " 0.37269375 0.15889656 0.2629594  0.71742963 0.1420697  0.6210084\n",
      " 0.80681374 0.77735901 0.48611367 0.44299345 0.79671838 0.33129217\n",
      " 0.4150646  0.64068885 0.70185878 0.28842461 0.84449403 0.72045506\n",
      " 0.29605544 0.53283996 0.40868775 0.97065628 0.58623193 0.45167509\n",
      " 0.76992482 0.55276866 0.70105256 0.15647343 0.3388488  0.33521587\n",
      " 0.18775887 0.42052316 0.38794756 0.12977091 0.66609209 0.46853529\n",
      " 0.17740996 0.86619775 0.65060532 0.17544326 0.78895334 0.41061038\n",
      " 0.17434802 0.04918724 0.72309707 0.6724519  0.26340022 0.494795\n",
      " 0.43792034 0.58040305 0.82172876 0.14972327 0.3121342  0.39648335\n",
      " 0.28783887 0.14154266 0.68047859 0.07354891 0.52219879 0.23249115\n",
      " 0.30631032 0.26972456 0.39237813 0.17742181 0.45160286 0.81945163\n",
      " 0.54350931 0.23492346 0.36497849 0.39255579 0.27463717 0.40946125\n",
      " 0.25173859 0.94633779 0.58179413 0.53435291 0.42216432 0.58742808\n",
      " 0.42044704 0.76857894 0.11010688 0.28482816 0.41476488 0.55354557\n",
      " 0.58055716 0.53154548 0.11281102 0.25033014 0.41549015 0.67855865\n",
      " 0.52178181 0.53884551 0.46386905 0.21094412 0.68636771 0.46554915\n",
      " 0.69459938 0.21284409 0.44049153 0.2561044  0.33323762 0.72878604\n",
      " 0.7273484  0.2061155  0.37142714 0.65360477 0.82334141 0.39253841\n",
      " 0.43592219 0.72161104 0.73743681 0.5924811  0.8284596  0.37787332\n",
      " 0.20899457 0.41000036 0.3316814  0.40811321 0.42865027 0.14234868\n",
      " 0.82990248 0.21489126 0.6741494  0.53396574 0.09745005 0.49148622\n",
      " 0.43403806 0.85428233 0.6142002  0.19898691 0.44720236 0.35666727\n",
      " 0.38109648 0.36165177 0.76001148 0.6121007  0.42508163 0.65988726\n",
      " 0.82530373 0.88639199 0.13389511 0.34083634 0.322658   0.47629076\n",
      " 0.12961515 0.70110497 0.7782112  0.26002596 0.27826746 0.31216331\n",
      " 0.48292052 0.79520051 0.60597194 0.53196011 0.0919431  0.9000772\n",
      " 0.31491021 0.79875104 0.39964281 0.50123456 0.42270565 0.33059508\n",
      " 0.77225014 0.80630208 0.37416936 0.11739556 0.60233821 0.63475469\n",
      " 0.22970043 0.7599628  0.26909432 0.63256581 0.76403762 0.18300323\n",
      " 0.94155009 0.26420876 0.71229176 0.61378395 0.44588316 0.80692376\n",
      " 0.63168664 0.73654433 0.77485897 0.59728338 0.48892342 0.1694312\n",
      " 0.23188737 0.33407451 0.48279072 0.23864268 0.93587934 0.70823706\n",
      " 0.36678668 0.85781235 0.22266929 0.10565667 0.70981416 0.67558313\n",
      " 0.61174666 0.79174877 0.1407362  0.54668581 0.35840953 0.48208093\n",
      " 0.52703647 0.82046072 0.6842374  0.41923107 0.46124068 0.37193607\n",
      " 0.66395023 0.7700125  0.35706356 0.86049115 0.58176905 0.29912903\n",
      " 0.43095597 0.24926115 0.27018662 0.58624497 0.33606616 0.63410793\n",
      " 0.75172712 0.33780885 0.44844811 0.16408671 0.6548997  0.7252992\n",
      " 0.72541606 0.1249075  0.290308   0.20497991 0.59917193 0.71449983\n",
      " 0.63876345 0.45310562 0.77947404 0.11625529 0.2753684  0.32746748\n",
      " 0.19233579 0.43674644 0.95091185 0.72726077 0.47538815 0.47937849\n",
      " 0.18063335 0.10983011 0.27679111 0.37264049 0.12913556 0.58679354\n",
      " 0.81623607 0.26719413 0.55839357 0.31751376 0.59224342 0.31691396\n",
      " 0.24707222 0.17257077 0.40434461 0.80773877 0.34065921 0.21721863\n",
      " 0.29574692 0.66609386 0.47228478 0.92306638 0.6339059  0.48207006\n",
      " 0.4182678  0.09873238 0.73196383 0.76925471 0.6308914  0.34348741\n",
      " 0.33395851 0.64349614 0.72681895 0.5697352  0.4029822  0.41146652\n",
      " 0.72657301 0.59059598 0.74725963 0.18356791 0.59463284 0.31651007\n",
      " 0.51976479 0.49607826 0.56643433 0.12873967 0.50657961 0.72846606\n",
      " 0.79785592 0.6815368  0.24446972 0.43308695]\n",
      "Objective Value at Optimal Position: 279.5096006937231\n"
     ]
    }
   ],
   "source": [
    "# Calling the modified PSO function\n",
    "best_pos_pso, best_val_pso = pso(lb=lb, ub=ub, swarmsize=100, maxiter=100)  # Kurangi untuk kecepatan\n",
    "\n",
    "print(f\"Optimal Position: {best_pos_pso}\")\n",
    "print(f\"Objective Value at Optimal Position: {best_val_pso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Emp2ujPb3jy"
   },
   "source": [
    "# Ant Colony Optimization (ACO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aco(lb, ub, ants=100, maxiter=100, alpha=1.0, beta=2.0, evaporation_rate=0.5, pheromone_deposit=0.1, debug=False):\n",
    "    \"\"\"\n",
    "    Perform an Ant Colony Optimization (ACO) without an external objective function.\n",
    "\n",
    "    Parameters:\n",
    "    - lb: The lower bounds of the design variable(s)\n",
    "    - ub: The upper bounds of the design variable(s)\n",
    "    - ants: The number of ants in the colony (Default: 100)\n",
    "    - maxiter: The maximum number of iterations (Default: 100)\n",
    "    - alpha: Relative importance of pheromone (Default: 1.0)\n",
    "    - beta: Relative importance of heuristic information (Default: 2.0)\n",
    "    - evaporation_rate: Rate at which pheromone evaporates (Default: 0.5)\n",
    "    - pheromone_deposit: Amount of pheromone deposited by ants (Default: 0.1)\n",
    "    - debug: If True, progress statements will be displayed (Default: False)\n",
    "\n",
    "    Returns:\n",
    "    - The best known position and objective value\n",
    "    \"\"\"\n",
    "    dim = len(lb)\n",
    "    pheromone_levels = np.ones((ants, dim))\n",
    "    best_val = np.inf\n",
    "    best_pos = None\n",
    "\n",
    "    def evaluate(position):\n",
    "        # Define the objective directly inside ACO. For demonstration, using sum of squares.\n",
    "        return np.sum(position**2)\n",
    "\n",
    "    # Main ACO loop\n",
    "    for iteration in range(maxiter):\n",
    "        positions = np.random.uniform(low=lb, high=ub, size=(ants, dim))\n",
    "        for ant in range(ants):\n",
    "            val = evaluate(positions[ant])  # Use the internal evaluate function\n",
    "            if val < best_val:\n",
    "                best_val = val\n",
    "                best_pos = positions[ant]\n",
    "\n",
    "            # Update pheromones\n",
    "            pheromone_levels[ant] += pheromone_deposit\n",
    "\n",
    "        # Evaporate pheromones\n",
    "        pheromone_levels *= (1 - evaporation_rate)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Iteration {iteration}: Best Value = {best_val}\")\n",
    "\n",
    "    return best_pos, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Position: [0.19691547 0.35158993 0.92117734 0.10252191 0.57493406 0.7361747\n",
      " 0.37994098 0.24434645 0.99141828 0.15037959 0.3136146  0.59097791\n",
      " 0.78702276 0.6272616  0.04126047 0.01877709 0.00535634 0.10991951\n",
      " 0.95273278 0.9614997  0.2143631  0.54187743 0.38206646 0.97725769\n",
      " 0.38436827 0.41866795 0.16300202 0.13162775 0.73165949 0.6958299\n",
      " 0.75791223 0.85832678 0.43024306 0.25406855 0.08122921 0.34723733\n",
      " 0.84320706 0.98252415 0.24880773 0.65356582 0.74249317 0.89658656\n",
      " 0.91523492 0.9717555  0.4605464  0.7539407  0.20766435 0.53487099\n",
      " 0.19281738 0.03925469 0.98723849 0.80787787 0.06489214 0.49246565\n",
      " 0.64510767 0.86824849 0.10859142 0.99688986 0.42033827 0.59750946\n",
      " 0.16532794 0.95865765 0.24444413 0.08199649 0.44766087 0.40689459\n",
      " 0.45658645 0.83158254 0.6432925  0.7342336  0.60746273 0.2864221\n",
      " 0.16155206 0.54702809 0.48266984 0.31626299 0.25024589 0.57891597\n",
      " 0.98821114 0.97657622 0.7346792  0.44841906 0.35016734 0.00784013\n",
      " 0.62124943 0.5756385  0.03968691 0.9109175  0.06380651 0.0021999\n",
      " 0.64326849 0.70597025 0.53271349 0.24909655 0.08457921 0.21941775\n",
      " 0.82416044 0.72160232 0.1607105  0.88385254 0.57185419 0.7899224\n",
      " 0.45564894 0.22755022 0.74899299 0.23793348 0.04280835 0.92302088\n",
      " 0.27473115 0.03721666 0.21166022 0.26441216 0.14659355 0.31527807\n",
      " 0.30198996 0.33011405 0.35636242 0.29647707 0.05425433 0.75573961\n",
      " 0.59183121 0.05514574 0.32361335 0.42196243 0.06992489 0.53950439\n",
      " 0.8538963  0.9166888  0.16796283 0.07744298 0.68176267 0.88443953\n",
      " 0.70654846 0.5344726  0.03962706 0.63366089 0.46374824 0.451178\n",
      " 0.03026726 0.89101768 0.7530895  0.34615498 0.6555644  0.29078223\n",
      " 0.43542897 0.88415076 0.2915704  0.41331757 0.28262799 0.69907088\n",
      " 0.95212469 0.35670761 0.65402154 0.05317378 0.01944785 0.65232994\n",
      " 0.12674197 0.71858392 0.3764318  0.35415289 0.52661422 0.25889494\n",
      " 0.20370163 0.37157497 0.94014553 0.96897394 0.09871637 0.18294615\n",
      " 0.03145863 0.09264305 0.56219478 0.26107802 0.11589491 0.71686344\n",
      " 0.6559393  0.3416924  0.25592825 0.9860208  0.34439789 0.78333696\n",
      " 0.97280239 0.66745158 0.32030009 0.53133187 0.83445517 0.57909864\n",
      " 0.48517986 0.43794607 0.85779598 0.11742996 0.69256409 0.50698843\n",
      " 0.38644453 0.11541765 0.62648094 0.60949782 0.64785615 0.88883872\n",
      " 0.44286006 0.19916024 0.93941555 0.66632055 0.94238344 0.12219848\n",
      " 0.93121493 0.69866427 0.02390883 0.04441773 0.0531814  0.13484953\n",
      " 0.96588114 0.11049859 0.67137914 0.04501051 0.78417346 0.07031308\n",
      " 0.17336558 0.16012382 0.93580145 0.87286663 0.01249955 0.77337857\n",
      " 0.21036183 0.90930609 0.91403071 0.10971524 0.41279429 0.03686974\n",
      " 0.96697629 0.15396374 0.03965976 0.59848452 0.93872197 0.69992041\n",
      " 0.58732098 0.38450769 0.78589258 0.61752693 0.49065017 0.62163863\n",
      " 0.87402963 0.0558882  0.40855978 0.48352779 0.21213872 0.235383\n",
      " 0.55388977 0.1528344  0.59336013 0.89108164 0.35123958 0.63508145\n",
      " 0.50929898 0.30532459 0.48516075 0.90450998 0.50937428 0.11595342\n",
      " 0.89877474 0.11580159 0.42428615 0.12950226 0.63124392 0.69711987\n",
      " 0.67777307 0.32891675 0.24508717 0.62042491 0.42794094 0.54906027\n",
      " 0.04798225 0.28476918 0.16776366 0.48786007 0.26193876 0.62530035\n",
      " 0.63485013 0.30618185 0.72552987 0.32237569 0.50644601 0.93476088\n",
      " 0.24892222 0.71914039 0.00744911 0.0532106  0.57955791 0.06176593\n",
      " 0.27842622 0.47148738 0.5375972  0.93826858 0.37111208 0.84393523\n",
      " 0.56508561 0.27891564 0.46893827 0.62283587 0.27742045 0.38459089\n",
      " 0.07961193 0.57647213 0.18915357 0.00298912 0.11095836 0.43161603\n",
      " 0.85367372 0.75011812 0.11339382 0.21577203 0.7158247  0.43083985\n",
      " 0.72561823 0.544205   0.06173524 0.45316418 0.60879971 0.18120841\n",
      " 0.71024553 0.27584389 0.25235773 0.82046401 0.90111677 0.11550915\n",
      " 0.26704048 0.35450433 0.72873932 0.97986774 0.63384128 0.78691336\n",
      " 0.42915076 0.4890625  0.47905192 0.77005249 0.08103245 0.16832806\n",
      " 0.44769333 0.45271878 0.96227459 0.5449006  0.8231568  0.5740408\n",
      " 0.01606079 0.5165701  0.83373483 0.4768058  0.10215878 0.38340918\n",
      " 0.81562438 0.06896488 0.08711382 0.73595358 0.52550783 0.70155746\n",
      " 0.08147176 0.61484118 0.4272696  0.97393806 0.39548766 0.73459616\n",
      " 0.90391756 0.59605825 0.29517316 0.05171498 0.38842911 0.04510825\n",
      " 0.06588171 0.57241935 0.90699627 0.49859665 0.06333531 0.71025195\n",
      " 0.69591807 0.3402223  0.47760119 0.02363566 0.198398   0.6157721\n",
      " 0.55904152 0.66299685 0.19182133 0.7783838  0.88665907 0.09348861\n",
      " 0.78490407 0.41886723 0.58769352 0.4568869  0.99511789 0.18217748\n",
      " 0.06475169 0.85964453 0.114197   0.47785852 0.2185564  0.33074477\n",
      " 0.51479445 0.56788353 0.29454702 0.26496804 0.89711842 0.14662062\n",
      " 0.07720286 0.12533378 0.69040717 0.63423289 0.28706636 0.12045042\n",
      " 0.7467796  0.88123545 0.03546182 0.68571376 0.24391083 0.69578064\n",
      " 0.45862029 0.05234803 0.49638413 0.02368399 0.84875497 0.66052783\n",
      " 0.98569443 0.64265379 0.00719936 0.59846718 0.36665373 0.76843806\n",
      " 0.22867846 0.0899422  0.14553981 0.74386367 0.04800614 0.07122595\n",
      " 0.57736651 0.08735337 0.03620409 0.65903634 0.21134272 0.13185502\n",
      " 0.46876525 0.18227361 0.14407826 0.21378289 0.02044643 0.08947092\n",
      " 0.32175404 0.27542525 0.35104126 0.5288481  0.35034564 0.77564183\n",
      " 0.02775242 0.75440155 0.48248267 0.89144828 0.06866698 0.2630962\n",
      " 0.59040497 0.63337371 0.15383851 0.55074462 0.78202359 0.60921816\n",
      " 0.25124183 0.79847709 0.80077708 0.58796888 0.62138221 0.37126254\n",
      " 0.70700626 0.77740738 0.72584336 0.01642985 0.82809823 0.55244837\n",
      " 0.68427056 0.53789943 0.61576911 0.27129073 0.36838849 0.98496527\n",
      " 0.82959495 0.42450657 0.15071705 0.71248617 0.76059925 0.10462537\n",
      " 0.20095199 0.5922355  0.14814554 0.56399854 0.11132757 0.55528922\n",
      " 0.79377307 0.83087609 0.24513146 0.12718571 0.50174694 0.42931076\n",
      " 0.32187255 0.44701183 0.4370696  0.43500358 0.64334489 0.81511342\n",
      " 0.51255109 0.36905018 0.26677375 0.36791158 0.86179727 0.35085638\n",
      " 0.96221644 0.01356014 0.46094761 0.05740013 0.27360912 0.29681338\n",
      " 0.48018245 0.10066312 0.63259441 0.40863667 0.494457   0.73634521\n",
      " 0.13219015 0.01183441 0.10773716 0.68082239 0.28691528 0.1249143\n",
      " 0.25219277 0.95652111 0.48352189 0.67688642 0.0519532  0.37923882\n",
      " 0.24892643 0.48049686 0.1617296  0.88731848 0.58629482 0.6649207\n",
      " 0.32851758 0.37510971 0.37969842 0.66716876 0.35488377 0.90456247\n",
      " 0.67136411 0.15645505 0.32642322 0.44014565 0.29789109 0.72906222\n",
      " 0.32633946 0.22733261 0.77392474 0.71397917 0.703572   0.1002519\n",
      " 0.28443653 0.40001912 0.90771573 0.39693487 0.45206689 0.45707784\n",
      " 0.44087414 0.33324332 0.8249852  0.31565951 0.07781323 0.20983239\n",
      " 0.31584386 0.37048699 0.07647658 0.66350396 0.24290407 0.70898356\n",
      " 0.60826537 0.75350731 0.83727988 0.33997094 0.60699589 0.28387805\n",
      " 0.02843516 0.4598846  0.25689174 0.74866667 0.61308348 0.80993499\n",
      " 0.04873168 0.11469362 0.90865514 0.4420949  0.34774973 0.19910065\n",
      " 0.99289436 0.15527361 0.19813625 0.66663248 0.52926157 0.80808272\n",
      " 0.54420445 0.115444   0.38433079 0.66706611 0.13128014 0.62850425\n",
      " 0.79994883 0.08493803 0.14777967 0.55358476 0.44208301 0.62885064\n",
      " 0.36590072 0.146926   0.5943638  0.81853257 0.05425307 0.69525749\n",
      " 0.69191948 0.5765146  0.95737206 0.42830609 0.57126161 0.743223\n",
      " 0.60586772 0.59278289 0.97968905 0.9363391  0.5942681  0.68838898\n",
      " 0.58948451 0.1427135  0.07834884 0.41247138 0.81257957 0.41799965\n",
      " 0.46067107 0.07635248 0.72038053 0.68057794 0.9802018  0.37832075\n",
      " 0.13303434 0.38568844 0.5720342  0.85431807 0.12471785 0.19751196\n",
      " 0.79644199 0.79720463 0.09260294 0.22484735 0.10095012 0.22044095\n",
      " 0.86013845 0.69515558 0.28274323 0.77242752 0.68621841 0.16002913\n",
      " 0.39018225 0.42329831 0.86920227 0.86263577 0.45308267 0.16451695\n",
      " 0.85915202 0.14984219 0.89599265 0.01668778 0.82622845 0.8485315\n",
      " 0.76321868 0.11784267 0.3852905  0.54391894 0.69341556 0.90542272\n",
      " 0.09056738 0.8586339  0.19730493 0.28202296 0.56383841 0.86576035\n",
      " 0.53424363 0.08000322 0.64418719 0.43475953 0.25768704 0.91445103\n",
      " 0.86430088 0.01952637 0.09725824 0.10881329 0.61105065 0.19779906\n",
      " 0.0822648  0.48586238 0.76608618 0.80681113 0.81823707 0.09859793\n",
      " 0.45702892 0.17329157 0.07494288 0.86052251 0.57989931 0.45117764\n",
      " 0.74280603 0.27229721 0.9728482  0.26589956 0.03371369 0.25597529\n",
      " 0.05218626 0.83790375 0.63474725 0.5571019  0.9745003  0.94049295\n",
      " 0.23029069 0.47153766 0.59370428 0.05928058 0.47572379 0.48455034\n",
      " 0.6522796  0.89461067 0.75816849 0.15227379 0.49482945 0.92206925\n",
      " 0.66240257 0.26604879 0.15525966 0.29738297 0.01767925 0.04459639\n",
      " 0.39674244 0.91794868 0.25404988 0.06098976 0.52934344 0.17254105\n",
      " 0.25821384 0.94720377 0.25731084 0.90039412 0.2364361  0.30492138\n",
      " 0.47074516 0.37818561 0.23105621 0.72331774 0.89543431 0.49763367\n",
      " 0.08669861 0.47088161 0.59765262 0.74732851 0.05266661 0.77200548\n",
      " 0.02164711 0.06059331 0.25539134 0.32045379 0.3104199  0.7546282\n",
      " 0.43725314 0.19872231 0.28398121 0.81789129 0.53356027 0.82937684\n",
      " 0.06074904 0.15707951 0.38438193 0.60889579 0.54287128 0.49288081\n",
      " 0.21524785 0.47199174 0.16970556 0.61740533 0.22414659 0.85811696\n",
      " 0.36702751 0.41238233 0.39808217 0.51744884 0.86533002 0.35032791\n",
      " 0.14614059 0.58180405 0.15901186 0.96793677 0.09802436 0.38937034\n",
      " 0.17365921 0.3688024  0.47284167 0.54008243 0.2832488  0.54951357\n",
      " 0.19905849 0.1312075  0.28755644 0.55763859 0.75810587 0.43420691\n",
      " 0.33656295 0.40340429 0.17451259 0.23946452 0.27941817 0.69232598\n",
      " 0.1757325  0.21071592 0.18854424 0.20421632 0.15896062 0.88059006\n",
      " 0.2375197  0.30119555 0.53596294 0.24706722 0.63342806 0.14203649\n",
      " 0.40488446 0.31195443 0.0135964  0.09685988 0.84218888 0.72208573\n",
      " 0.05138825 0.50203917 0.68735895 0.52903768 0.30919147 0.11361726\n",
      " 0.02411437 0.82426615 0.05708906 0.51753195 0.50292399 0.21092791\n",
      " 0.09708522 0.34281708 0.6400643  0.98518125 0.70337305 0.51805211\n",
      " 0.90120763 0.79308967 0.94160055 0.02254953 0.72115974 0.1121358\n",
      " 0.69280062 0.84799762 0.88974875 0.81570803 0.76259421 0.99392921\n",
      " 0.44390687 0.66346442 0.50475807 0.51434632 0.1160537  0.38339099\n",
      " 0.79385159 0.36085116 0.43978568 0.37877596 0.85974313 0.47027865\n",
      " 0.1270945  0.2860267  0.8001494  0.23583674 0.91622503 0.51301286\n",
      " 0.46706475 0.71064601 0.50935794 0.82581467 0.79254945 0.53533809\n",
      " 0.70959625 0.10299639 0.66445506 0.64655227 0.12571416 0.22545846\n",
      " 0.52428238 0.73465645 0.81979758 0.3684971  0.44766699 0.37213152\n",
      " 0.02090542 0.58591574 0.80693481 0.24842573 0.85999763 0.31923675\n",
      " 0.74781807 0.23323448 0.20762984 0.48159785 0.43426078 0.0419945\n",
      " 0.02051258 0.26676404 0.86459843 0.70094545 0.50058612 0.98692398\n",
      " 0.63658987 0.45334906 0.30219179 0.49191552 0.01449655 0.75650383\n",
      " 0.24073455 0.3559511  0.54758931 0.24486483 0.97853647 0.49968877\n",
      " 0.41795971 0.08861119 0.28048544 0.43353976 0.15313365 0.55643258\n",
      " 0.17350847 0.48298786 0.69897356 0.88318263 0.149859   0.07318746\n",
      " 0.65097798 0.19140356 0.52264935 0.20590372 0.90764953 0.84458707\n",
      " 0.074429   0.11608285 0.39557484 0.30561842 0.20397872 0.43303396\n",
      " 0.90609602 0.90597566 0.33394601 0.39397586 0.14900188 0.46414978\n",
      " 0.1281357  0.93010713 0.15687023 0.74595654 0.98551614 0.21805362\n",
      " 0.54880915 0.42309757 0.92005264 0.43015887 0.04425004 0.99955539\n",
      " 0.10646397 0.37453913 0.20348143 0.87027016 0.19242985 0.13572074\n",
      " 0.66608728 0.67457584 0.27656546 0.01224854 0.62553512 0.37861907\n",
      " 0.42535727 0.17429636 0.76825218 0.11584886 0.80113589 0.28694981\n",
      " 0.72423404 0.54835642 0.09540303 0.61560339 0.1110328  0.36550824\n",
      " 0.97275086 0.67353537 0.84464974 0.66813553]\n",
      "Objective Value at Optimal Position: 296.20668124131385\n"
     ]
    }
   ],
   "source": [
    "# Calling the modified PSO function\n",
    "best_pos_aco, best_val_aco = aco(lb=lb, ub=ub, ants=100, maxiter=100)\n",
    "\n",
    "print(f\"Optimal Position: {best_pos_aco}\")\n",
    "print(f\"Objective Value at Optimal Position: {best_val_aco}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBNlyX46b7fb"
   },
   "source": [
    "# Cat Swarm Optimization (CSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "S2gAPTPEb_JR"
   },
   "outputs": [],
   "source": [
    "def cso(lb, ub, cats=100, maxiter=100, mix_rate=0.5, seeking_memory_pool=5, seeking_range_of_selected_dimension=0.2, counts_of_dimension_to_change=2, debug=False):\n",
    "    \"\"\"\n",
    "    Perform a Cat Swarm Optimization (CSO) without an external objective function.\n",
    "\n",
    "    Parameters:\n",
    "    - lb: The lower bounds of the design variable(s)\n",
    "    - ub: The upper bounds of the design variable(s)\n",
    "    - cats: The number of cats in the swarm (Default: 100)\n",
    "    - maxiter: The maximum number of iterations (Default: 100)\n",
    "    - mix_rate: Mixture rate to switch between seeking and tracing modes (Default: 0.5)\n",
    "    - seeking_memory_pool: Size of memory pool in seeking mode (Default: 5)\n",
    "    - seeking_range_of_selected_dimension: Range of selected dimension in seeking mode (Default: 0.2)\n",
    "    - counts_of_dimension_to_change: Number of dimensions to change in seeking mode (Default: 2)\n",
    "    - debug: If True, progress statements will be displayed (Default: False)\n",
    "\n",
    "    Returns:\n",
    "    - The best known position and objective value\n",
    "    \"\"\"\n",
    "    dim = len(lb)\n",
    "    best_val = np.inf\n",
    "    best_pos = None\n",
    "    positions = np.random.uniform(low=lb, high=ub, size=(cats, dim))\n",
    "\n",
    "    def evaluate(position):\n",
    "        # Define the objective directly inside CSO. For demonstration, using sum of squares.\n",
    "        return np.sum(position**2)\n",
    "\n",
    "    # Main CSO loop\n",
    "    for iteration in range(maxiter):\n",
    "        for cat in range(cats):\n",
    "            if np.random.rand() < mix_rate:\n",
    "                # Seeking mode\n",
    "                for _ in range(seeking_memory_pool):\n",
    "                    candidate_position = positions[cat] + np.random.uniform(-1, 1, size=dim) * seeking_range_of_selected_dimension\n",
    "                    candidate_position = np.clip(candidate_position, lb, ub)\n",
    "                    val = evaluate(candidate_position)  # Use the internal evaluate function\n",
    "                    if val < best_val:\n",
    "                        best_val = val\n",
    "                        best_pos = candidate_position\n",
    "            else:\n",
    "                # Tracing mode (simplified as random walk in this example)\n",
    "                positions[cat] += np.random.uniform(-1, 1, size=dim)\n",
    "                positions[cat] = np.clip(positions[cat], lb, ub)\n",
    "                val = evaluate(positions[cat])  # Use the internal evaluate function\n",
    "                if val < best_val:\n",
    "                    best_val = val\n",
    "                    best_pos = positions[cat]\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Iteration {iteration}: Best Value = {best_val}\")\n",
    "\n",
    "    return best_pos, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dI-hqeWVc98G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Position: [0.81826583 0.33427775 0.76507779 0.69836002 0.71104877 0.06134806\n",
      " 0.72190407 0.02812705 0.59048766 0.5311182  0.8286445  0.96202035\n",
      " 0.         0.23776484 0.21333852 0.20215863 0.15841281 0.97497728\n",
      " 0.13985121 0.21306554 0.1188011  0.         0.73563864 0.96608398\n",
      " 0.99702987 0.51055158 0.77976659 0.00240278 0.83366392 0.2998048\n",
      " 0.6317847  0.20310213 0.74116226 0.24279657 1.         0.47581504\n",
      " 0.05831978 0.83810079 0.70356862 0.51149586 0.65635207 0.29865283\n",
      " 0.85147655 0.62933294 0.29672228 0.         0.48366284 0.09133543\n",
      " 1.         0.21271731 0.50029436 0.94217084 0.88682571 0.95421887\n",
      " 0.28358025 0.76887235 0.8546622  0.25101675 0.63626442 0.11053425\n",
      " 0.84242426 0.88474396 0.62344376 0.34270617 0.21397351 0.\n",
      " 0.44352359 0.81147033 0.4579802  0.69511    0.89369629 0.55384003\n",
      " 0.23238559 0.30755496 0.60403851 0.51970023 0.38308325 0.02870732\n",
      " 0.26185649 0.37394818 0.62139755 0.01230802 0.89304189 0.20136246\n",
      " 0.64748951 0.68894493 0.21342344 0.21140755 0.69790441 0.4156381\n",
      " 0.36166085 0.34738919 0.32462888 0.21562268 0.35310589 0.47718715\n",
      " 0.07764896 0.35475992 0.48595598 0.74327974 0.05401334 0.08855747\n",
      " 0.60290602 0.28341811 0.62133688 0.         0.24120858 0.22556182\n",
      " 0.39796352 0.54099353 0.46772413 0.51783016 0.85471633 0.32100649\n",
      " 0.         0.29420384 0.72079402 0.64344174 0.3118595  0.62430711\n",
      " 0.08453872 0.70068132 0.624532   0.02063854 0.41776332 0.13298827\n",
      " 0.99167973 1.         0.4600597  0.61354501 0.         0.24348382\n",
      " 0.         0.269328   0.14769257 0.86232141 0.41080694 0.08553884\n",
      " 0.         0.26241261 0.0761542  0.35151142 0.51584583 0.52914083\n",
      " 0.39907587 0.1355348  0.15587235 0.10079346 0.79917112 0.6744143\n",
      " 0.         0.         1.         0.46848368 0.93480301 0.86194332\n",
      " 0.80110997 0.43082688 0.18910431 0.82638172 0.04269027 0.65111095\n",
      " 0.0098738  0.88825598 0.28369245 0.07345652 0.59537979 1.\n",
      " 1.         0.44291621 0.56438715 0.89879959 0.6570032  0.\n",
      " 0.42134442 0.40101464 0.07390126 0.37002471 0.5494993  0.73114533\n",
      " 0.42686138 0.34343351 0.06719289 0.38605348 0.39837982 0.\n",
      " 0.9554091  0.58998629 0.61306847 0.4580881  0.12946953 0.76607723\n",
      " 0.6150449  0.77733459 0.62766923 0.13216042 0.65757616 0.5758728\n",
      " 0.91690026 0.51785902 0.4153554  0.75969109 0.82668147 0.69321353\n",
      " 0.28020109 0.63524965 1.         0.17549252 0.17378358 0.32040597\n",
      " 0.68168903 0.49452759 1.         0.79869285 0.23798073 0.19564285\n",
      " 0.70518228 0.44192512 0.32961746 0.71246964 0.5861272  0.14802732\n",
      " 0.13968321 0.6693808  0.43033832 0.69504394 0.24856958 0.53606701\n",
      " 0.63814734 0.         0.66597334 0.43045405 0.18666147 0.22370949\n",
      " 0.39887746 0.69504652 0.45153922 0.36859854 0.09737141 0.84706019\n",
      " 0.95234561 0.39867601 0.14856668 0.61631766 0.52955703 1.\n",
      " 1.         0.89372422 0.63094021 0.21140455 0.42038963 0.3057487\n",
      " 0.59015581 0.24133626 0.         0.79433336 0.43991666 0.07116719\n",
      " 0.3862402  0.         0.40861292 0.89225059 0.63825325 0.3999447\n",
      " 0.21559332 0.76861405 0.91022809 0.45599513 0.3669841  0.32086332\n",
      " 0.31843073 0.99251696 0.39324478 0.27158916 0.71772406 0.87168488\n",
      " 0.57339231 0.72229102 0.30991429 0.52976773 0.         0.34661943\n",
      " 1.         0.25378493 0.16519549 0.25904329 0.25709179 0.59168213\n",
      " 0.97285975 0.23523832 0.38273513 0.34677685 0.59250195 0.80517283\n",
      " 0.         0.64614333 0.66296393 0.09300907 0.36392359 0.75854923\n",
      " 1.         0.43534336 0.44395669 0.35540418 0.0171907  0.82539766\n",
      " 0.52026499 0.73917254 0.01527474 0.80005931 0.75917786 0.18920114\n",
      " 0.         0.52350414 0.63293404 0.2980906  0.76229351 0.03001343\n",
      " 0.50568677 0.69526012 0.         0.72194883 0.81750174 0.08067424\n",
      " 0.83082967 0.48725299 0.59552088 0.14356959 0.55871875 0.31855047\n",
      " 0.13030162 0.76747846 0.28133924 0.25369555 1.         0.24150101\n",
      " 0.59030251 0.86089181 1.         0.62551618 0.60236225 0.53955766\n",
      " 0.         0.22274545 1.         0.19718075 0.7325614  0.46217389\n",
      " 0.30045047 0.02851382 0.47515782 0.73807573 0.22230674 0.81543721\n",
      " 0.37575474 0.28570293 0.74971965 0.76763804 0.74849786 0.55147114\n",
      " 0.07980046 0.         0.36356332 0.27354496 0.09478567 0.72634883\n",
      " 0.89597692 0.14220681 0.82852019 0.35972893 0.28114449 0.\n",
      " 0.23800809 0.24370896 0.18771382 0.22989709 0.47673357 0.30826585\n",
      " 0.41562064 0.1377287  0.76998931 0.83955159 0.64258998 1.\n",
      " 0.50391259 0.         0.73603784 0.67545101 0.         0.20327574\n",
      " 0.45136057 0.35752313 0.57790754 0.67812887 0.5626103  0.70742295\n",
      " 1.         0.72168765 0.75024618 0.44938966 0.30904226 0.78706982\n",
      " 0.31229891 0.12003233 0.32671758 0.76515857 0.37928585 0.53079892\n",
      " 0.62240214 0.84055459 0.80587872 0.74071308 0.62120036 0.15356329\n",
      " 0.62090476 0.14334629 0.59141996 0.34922268 0.49477619 0.47307099\n",
      " 0.83935494 0.7991849  0.4748742  0.39498391 0.09327299 1.\n",
      " 0.42407473 0.28844384 0.53327686 0.3521799  0.24437847 0.10343372\n",
      " 0.6101532  0.35071931 0.23023507 0.06379008 0.25795881 0.49498238\n",
      " 1.         0.76054039 1.         1.         0.35272012 0.66714252\n",
      " 0.84943975 0.96002458 0.33176122 0.         0.49779357 0.25869995\n",
      " 0.24080213 0.73831088 0.67215088 0.43601633 0.59982861 0.51046433\n",
      " 0.25345826 1.         0.90127049 0.22154043 0.08616808 0.44099452\n",
      " 0.96154096 0.10999464 0.48852013 0.         0.64121246 0.\n",
      " 0.19288727 0.87063424 0.35074303 0.75096103 0.         0.63761632\n",
      " 0.95355742 0.6417768  0.43120295 0.03135454 0.41475012 0.64697399\n",
      " 0.33051862 0.56297348 0.39367249 0.48912078 0.70432706 0.18334211\n",
      " 0.27682362 0.62936916 0.5592087  0.65410744 0.42214865 0.67997446\n",
      " 0.56988905 0.82525175 0.4446979  0.30902243 0.78413152 0.12574141\n",
      " 0.21486399 0.25541771 0.82482247 0.07291391 0.93625319 0.48393494\n",
      " 1.         0.48834832 0.86080435 0.40334858 1.         0.26751793\n",
      " 0.73628185 0.8794728  0.51191799 0.6970138  0.11365017 0.51024546\n",
      " 0.24859488 0.46847849 0.09053553 0.08999417 0.13404889 0.\n",
      " 0.40542293 0.61246039 0.75570953 0.13247173 0.7451685  0.80354397\n",
      " 0.29189037 0.8757204  0.51380383 0.80397777 0.0125583  0.69472417\n",
      " 0.45126829 0.07855375 0.2516684  0.70241911 0.02868863 0.\n",
      " 0.59811356 0.82713169 0.2254407  0.51113384 0.36717824 0.23187064\n",
      " 0.39061359 0.45153203 0.         1.         0.31612416 0.98992497\n",
      " 0.         0.49978543 0.89360024 0.47656595 0.16699117 0.66408251\n",
      " 0.95958393 0.40295322 0.14912175 0.13308807 0.4453915  0.\n",
      " 0.58753195 0.27364872 0.68585844 0.02782449 0.60967606 0.76989801\n",
      " 0.87077751 0.685235   0.44216903 0.02308911 0.2851445  0.78285956\n",
      " 0.88826208 0.71012059 0.03079457 0.83796553 0.19232313 0.66210541\n",
      " 0.         0.67520666 0.28249377 0.81128455 0.60840033 0.79892288\n",
      " 1.         0.95526183 0.58730582 0.12727943 0.37094556 0.5197557\n",
      " 0.77868802 0.80985288 0.53664859 1.         0.35020383 0.73986572\n",
      " 0.68926746 0.13737416 0.71136145 0.8543144  0.65685382 0.37645817\n",
      " 0.23627436 0.18171558 0.95790758 0.59295303 0.46488916 0.12683959\n",
      " 0.46124217 0.18862996 0.76358383 0.04336213 0.15239104 0.09148505\n",
      " 0.13211937 0.54997464 0.15574342 0.94645491 0.15468009 0.64181098\n",
      " 0.29290977 0.81891387 0.18484453 0.10097861 1.         0.49039564\n",
      " 0.55230374 0.30303738 0.         0.92549809 0.36050847 0.68584059\n",
      " 0.27361863 0.47709543 0.62882636 0.75804495 0.24261746 0.7741362\n",
      " 0.15254204 0.40464001 0.8103997  0.29934862 0.53588939 0.15811261\n",
      " 0.7824579  0.456035   0.48347395 0.29482912 0.26976031 0.03760818\n",
      " 0.59457319 0.68520086 0.65027022 0.42750566 0.76718667 0.07516396\n",
      " 0.34781011 0.94806975 0.35895148 0.48519849 0.70576623 0.2445753\n",
      " 0.56063426 0.4013097  0.35096654 0.79630431 0.14100686 1.\n",
      " 0.08749862 0.24992936 0.70907272 0.83517551 1.         0.18912432\n",
      " 0.         0.65833618 0.23781139 0.67798212 0.51965988 0.73641821\n",
      " 0.66139599 1.         0.         0.71367718 0.4151225  0.98476802\n",
      " 0.55362459 0.90988766 0.62392633 0.35088653 0.30532577 0.58013367\n",
      " 0.51923443 0.         0.96682077 0.4910216  0.29227582 0.56778449\n",
      " 0.21196945 0.01343352 0.22480123 0.84463138 0.         0.89595569\n",
      " 0.31569359 0.80060457 0.02933004 0.         0.60000044 0.\n",
      " 0.75603165 0.17999397 0.75807915 0.74699127 0.0171482  0.52614603\n",
      " 0.84003609 0.28722697 0.62354103 0.67485608 0.26688632 0.73681988\n",
      " 0.10619232 0.9666435  0.57128727 0.40089603 0.8220835  0.7278241\n",
      " 0.41271657 0.41019531 0.77743488 0.57234198 0.07457682 0.20189396\n",
      " 0.07975953 0.71766956 0.3090455  0.55862771 0.19421123 0.\n",
      " 0.48617969 0.08169743 1.         0.13235237 0.79321372 0.34496375\n",
      " 0.82156655 0.861298   0.8503212  0.74732386 0.57301912 0.23220102\n",
      " 0.40002373 0.83999228 0.89721813 1.         0.69277994 0.13921248\n",
      " 0.32534432 0.58245063 0.68698735 0.48774796 0.         0.60118955\n",
      " 0.30214816 0.67062541 0.2952785  0.10555343 0.44703858 0.9332998\n",
      " 0.6233106  0.82852482 1.         0.53075142 0.51583877 0.4109778\n",
      " 0.41732402 0.24333892 0.12524073 0.55087661 0.6504932  0.39544533\n",
      " 0.20759531 0.21093704 0.16673747 0.52070706 0.94274139 0.64617194\n",
      " 0.34466159 0.67983689 0.         0.32089776 0.12833525 0.2839638\n",
      " 0.52605286 0.57979953 0.73522126 0.40914049 0.08439321 0.16112027\n",
      " 0.08892055 0.29681726 0.90496871 0.51010216 0.8029806  0.43066207\n",
      " 0.92871786 0.20330384 0.37332002 0.         0.4960207  0.60006501\n",
      " 0.92285976 0.38830645 0.52099939 0.43697315 0.62175709 0.87636137\n",
      " 0.33672112 0.47930988 0.12083997 0.43709693 0.72517653 0.70175811\n",
      " 1.         0.40871452 0.6807365  0.50363323 0.44302156 0.17324975\n",
      " 0.45999441 0.22776967 0.24373352 0.35362383 0.29403287 0.77009761\n",
      " 0.98507766 0.53540204 0.83949274 0.56815933 0.34431702 0.53860968\n",
      " 0.35269864 0.67038447 0.1350346  0.60383547 0.25981769 1.\n",
      " 0.49483165 0.15139882 0.60062091 0.40286723 0.00654376 0.75756992\n",
      " 0.12698843 1.         0.95480968 0.91422538 0.76739343 1.\n",
      " 0.38336442 0.06912183 0.90474208 0.60552486 0.23382227 0.39178499\n",
      " 0.15291502 1.         0.         0.39901454 0.10119445 0.66892218\n",
      " 0.52108402 0.46307133 0.24484272 0.51775919 0.44796457 0.23310512\n",
      " 0.91443863 0.06477485 0.33743139 0.33405228 0.23155345 0.21455009\n",
      " 0.60400827 0.4844771  0.33481139 0.20380676 0.65173188 0.14707435\n",
      " 0.63150672 0.         0.34154607 1.         0.09532048 0.27219116\n",
      " 0.79580819 0.49098923 0.61866276 0.60254116 0.38949261 0.07397318\n",
      " 1.         0.20586542 0.4435296  0.17508147 0.94681443 0.58931699\n",
      " 0.09069932 0.2128102  0.13651196 0.51395056 0.52486963 0.35215186\n",
      " 0.79610812 0.63866693 1.         0.31704337 0.60480926 0.1076\n",
      " 0.45876639 0.89807602 0.23857982 0.         0.         0.51564445\n",
      " 0.15960689 0.61433268 0.06339577 0.77760382 0.57513807 0.46045698\n",
      " 0.56958807 0.52135013 0.4466563  1.         0.24867556 0.28848896\n",
      " 0.59646968 0.26485051 0.49615807 0.22859542 0.31846569 0.75580075\n",
      " 0.89988123 0.54116327 0.20119583 0.34465011 0.62433344 0.77447391\n",
      " 0.41166368 0.58315876 0.09611287 0.52533226 0.38220828 0.19695497\n",
      " 0.27688047 0.74392108 1.         0.51374983 0.24457997 0.32917573\n",
      " 0.39301976 0.9741019  0.47322462 0.87979331 0.31441076 0.60526717\n",
      " 0.31371897 0.6753984  0.53423116 0.23640292 0.68323669 0.92185874\n",
      " 0.76352313 0.39327116 0.2725283  0.6871716  0.78613875 0.83317136\n",
      " 0.85440293 0.07020112 0.73105973 0.10461472 0.0672437  0.3514165\n",
      " 0.76172162 0.51478572 0.09406172 0.94506304 0.24103018 0.6164097\n",
      " 0.19813663 0.52773402 0.62955068 0.73763386]\n",
      "Objective Value at Optimal Position: 313.5960949452037\n"
     ]
    }
   ],
   "source": [
    "best_pos_cso, best_val_cso = cso(lb=lb, ub=ub, cats=100, maxiter=100)\n",
    "\n",
    "print(f\"Optimal Position: {best_pos_cso}\")\n",
    "print(f\"Objective Value at Optimal Position: {best_val_cso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiMhM9xmdDFJ"
   },
   "source": [
    "# **Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HuWcVlqQjGR7"
   },
   "outputs": [],
   "source": [
    "def create_lstm_model(input_length, num_classes):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=5000, output_dim=50, input_length=input_length),\n",
    "        LSTM(50, dropout=0.4, recurrent_dropout=0.4),\n",
    "        Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid') # Gunakan 'sigmoid' untuk binary, 'softmax' untuk multiclass\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy' if num_classes > 2 else 'binary_crossentropy', metrics=['accuracy']) # Sesuaikan loss function\n",
    "    return model\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkdqyiVUfEzq"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AF4MgIqijKn8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\krist\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\krist\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\krist\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\krist\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "99/99 [==============================] - 62s 603ms/step - loss: 0.9164 - accuracy: 0.5966 - val_loss: 0.7878 - val_accuracy: 0.6654\n",
      "Epoch 2/10\n",
      "99/99 [==============================] - 58s 591ms/step - loss: 0.6429 - accuracy: 0.7363 - val_loss: 0.5843 - val_accuracy: 0.7721\n",
      "Epoch 3/10\n",
      "99/99 [==============================] - 58s 585ms/step - loss: 0.3889 - accuracy: 0.8598 - val_loss: 0.5313 - val_accuracy: 0.8089\n",
      "Epoch 4/10\n",
      "99/99 [==============================] - 58s 587ms/step - loss: 0.2297 - accuracy: 0.9276 - val_loss: 0.5345 - val_accuracy: 0.8305\n",
      "Epoch 5/10\n",
      "99/99 [==============================] - 58s 590ms/step - loss: 0.1564 - accuracy: 0.9536 - val_loss: 0.5790 - val_accuracy: 0.8337\n",
      "Epoch 6/10\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9686Restoring model weights from the end of the best epoch: 3.\n",
      "99/99 [==============================] - 60s 606ms/step - loss: 0.1153 - accuracy: 0.9686 - val_loss: 0.6573 - val_accuracy: 0.8305\n",
      "Epoch 6: early stopping\n",
      "62/62 [==============================] - 3s 46ms/step - loss: 0.5760 - accuracy: 0.7974\n",
      "Test Loss: 0.5759505033493042\n",
      "Test Accuracy: 0.797359049320221\n",
      "Time Execution: 354.71002864837646\n"
     ]
    }
   ],
   "source": [
    "model_lstm = create_lstm_model(X_train.shape[1], y_train.shape[1])\n",
    "\n",
    "start_time_lstm = time.time()\n",
    "history_lstm = model_lstm.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=64, callbacks=[early_stopping])\n",
    "end_time_lstm = time.time()\n",
    "time_execution_lstm = end_time_lstm - start_time_lstm\n",
    "\n",
    "# Evaluasi model\n",
    "loss_lstm, accuracy_lstm = model_lstm.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss_lstm}\")\n",
    "print(f\"Test Accuracy: {accuracy_lstm}\")\n",
    "print(f\"Time Execution: {time_execution_lstm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zY28ggTJezn8"
   },
   "source": [
    "# PSO-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "99/99 [==============================] - 25s 226ms/step - loss: 0.9415 - accuracy: 0.5706 - val_loss: 0.8465 - val_accuracy: 0.6305\n",
      "Epoch 2/10\n",
      "99/99 [==============================] - 22s 220ms/step - loss: 0.7759 - accuracy: 0.6647 - val_loss: 0.7494 - val_accuracy: 0.6603\n",
      "Epoch 3/10\n",
      "99/99 [==============================] - 22s 218ms/step - loss: 0.6070 - accuracy: 0.7454 - val_loss: 0.7067 - val_accuracy: 0.6832\n",
      "Epoch 4/10\n",
      "99/99 [==============================] - 22s 219ms/step - loss: 0.4814 - accuracy: 0.8081 - val_loss: 0.7519 - val_accuracy: 0.6940\n",
      "Epoch 5/10\n",
      "99/99 [==============================] - 21s 217ms/step - loss: 0.3967 - accuracy: 0.8446 - val_loss: 0.8407 - val_accuracy: 0.6940\n",
      "Epoch 6/10\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.3464 - accuracy: 0.8616Restoring model weights from the end of the best epoch: 3.\n",
      "99/99 [==============================] - 21s 217ms/step - loss: 0.3464 - accuracy: 0.8616 - val_loss: 0.9165 - val_accuracy: 0.6889\n",
      "Epoch 6: early stopping\n",
      "62/62 [==============================] - 1s 20ms/step - loss: 0.7377 - accuracy: 0.6755\n",
      "Test Loss: 0.7377488613128662\n",
      "Test Accuracy: 0.6754697561264038\n",
      "Time Execution: 132.95318913459778\n"
     ]
    }
   ],
   "source": [
    "# Asumsikan best_weights sudah ada\n",
    "selected_indices_pso = np.where(best_pos_pso > 0.5)[0]  # Ambil indeks dengan bobot > 0.5\n",
    "\n",
    "# Memfilter X_train dan X_test berdasarkan fitur terpilih\n",
    "X_train_selected_pso = X_train[:, selected_indices_pso]\n",
    "X_test_selected_pso = X_test[:, selected_indices_pso]\n",
    "\n",
    "model_pso_lstm = create_lstm_model(X_train_selected_pso.shape[1], y_train.shape[1])\n",
    "\n",
    "start_time_pso_lstm = time.time()\n",
    "history_pso_lstm = model_pso_lstm.fit(X_train_selected_pso, y_train, validation_split=0.2, epochs=10, batch_size=64, callbacks=[early_stopping])\n",
    "end_time_pso_lstm = time.time()\n",
    "time_execution_pso_lstm = end_time_pso_lstm - start_time_pso_lstm\n",
    "\n",
    "# Evaluasi model\n",
    "loss_pso_lstm, accuracy_pso_lstm = model_pso_lstm.evaluate(X_test_selected_pso, y_test)\n",
    "print(f\"Test Loss: {loss_pso_lstm}\")\n",
    "print(f\"Test Accuracy: {accuracy_pso_lstm}\")\n",
    "print(f\"Time Execution: {time_execution_pso_lstm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzSNkG_2e8Mj",
    "tags": []
   },
   "source": [
    "# ACO-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "99/99 [==============================] - 23s 212ms/step - loss: 0.9308 - accuracy: 0.5818 - val_loss: 0.8176 - val_accuracy: 0.6463\n",
      "Epoch 2/10\n",
      "99/99 [==============================] - 20s 203ms/step - loss: 0.6744 - accuracy: 0.7231 - val_loss: 0.6535 - val_accuracy: 0.7213\n",
      "Epoch 3/10\n",
      "99/99 [==============================] - 20s 204ms/step - loss: 0.4625 - accuracy: 0.8192 - val_loss: 0.6173 - val_accuracy: 0.7625\n",
      "Epoch 4/10\n",
      "99/99 [==============================] - 21s 213ms/step - loss: 0.3169 - accuracy: 0.8927 - val_loss: 0.6352 - val_accuracy: 0.7721\n",
      "Epoch 5/10\n",
      "99/99 [==============================] - 21s 213ms/step - loss: 0.2235 - accuracy: 0.9249 - val_loss: 0.6917 - val_accuracy: 0.7765\n",
      "Epoch 6/10\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.9438Restoring model weights from the end of the best epoch: 3.\n",
      "99/99 [==============================] - 21s 212ms/step - loss: 0.1749 - accuracy: 0.9438 - val_loss: 0.8086 - val_accuracy: 0.7543\n",
      "Epoch 6: early stopping\n",
      "62/62 [==============================] - 1s 21ms/step - loss: 0.6664 - accuracy: 0.7466\n",
      "Test Loss: 0.6664141416549683\n",
      "Test Accuracy: 0.7465718388557434\n",
      "Time Execution: 126.53417634963989\n"
     ]
    }
   ],
   "source": [
    "# Asumsikan best_weights sudah ada\n",
    "selected_indices_aco = np.where(best_pos_aco > 0.5)[0]  # Ambil indeks dengan bobot > 0.5\n",
    "\n",
    "# Memfilter X_train dan X_test berdasarkan fitur terpilih\n",
    "X_train_selected_aco = X_train[:, selected_indices_aco]\n",
    "X_test_selected_aco = X_test[:, selected_indices_aco]\n",
    "\n",
    "model_aco_lstm = create_lstm_model(X_train_selected_aco.shape[1], y_train.shape[1])\n",
    "\n",
    "start_time_aco_lstm = time.time()\n",
    "history_aco_lstm = model_aco_lstm.fit(X_train_selected_aco, y_train, validation_split=0.2, epochs=10, batch_size=64, callbacks=[early_stopping])\n",
    "end_time_aco_lstm = time.time()\n",
    "time_execution_aco_lstm = end_time_aco_lstm - start_time_aco_lstm\n",
    "\n",
    "# Evaluasi model\n",
    "loss_aco_lstm, accuracy_aco_lstm = model_aco_lstm.evaluate(X_test_selected_aco, y_test)\n",
    "print(f\"Test Loss: {loss_aco_lstm}\")\n",
    "print(f\"Test Accuracy: {accuracy_aco_lstm}\")\n",
    "print(f\"Time Execution: {time_execution_aco_lstm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wX8D2xXle8bx"
   },
   "source": [
    "# CSO-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "99/99 [==============================] - 25s 226ms/step - loss: 0.9363 - accuracy: 0.5674 - val_loss: 0.7993 - val_accuracy: 0.6629\n",
      "Epoch 2/10\n",
      "99/99 [==============================] - 22s 222ms/step - loss: 0.6802 - accuracy: 0.7130 - val_loss: 0.6533 - val_accuracy: 0.7302\n",
      "Epoch 3/10\n",
      "99/99 [==============================] - 22s 218ms/step - loss: 0.4473 - accuracy: 0.8300 - val_loss: 0.6014 - val_accuracy: 0.7594\n",
      "Epoch 4/10\n",
      "99/99 [==============================] - 22s 218ms/step - loss: 0.3047 - accuracy: 0.8963 - val_loss: 0.6682 - val_accuracy: 0.7879\n",
      "Epoch 5/10\n",
      "99/99 [==============================] - 22s 218ms/step - loss: 0.2280 - accuracy: 0.9255 - val_loss: 0.7325 - val_accuracy: 0.7905\n",
      "Epoch 6/10\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.1785 - accuracy: 0.9433Restoring model weights from the end of the best epoch: 3.\n",
      "99/99 [==============================] - 21s 217ms/step - loss: 0.1785 - accuracy: 0.9433 - val_loss: 0.8047 - val_accuracy: 0.7879\n",
      "Epoch 6: early stopping\n",
      "62/62 [==============================] - 1s 22ms/step - loss: 0.6624 - accuracy: 0.7390\n",
      "Test Loss: 0.6623703241348267\n",
      "Test Accuracy: 0.7389537692070007\n",
      "Time Execution: 132.79375791549683\n"
     ]
    }
   ],
   "source": [
    "# Asumsikan best_weights sudah ada\n",
    "selected_indices_cso = np.where(best_pos_cso > 0.5)[0]  # Ambil indeks dengan bobot > 0.5\n",
    "\n",
    "# Memfilter X_train dan X_test berdasarkan fitur terpilih\n",
    "X_train_selected_cso = X_train[:, selected_indices_cso]\n",
    "X_test_selected_cso = X_test[:, selected_indices_cso]\n",
    "\n",
    "model_cso_lstm = create_lstm_model(X_train_selected_cso.shape[1], y_train.shape[1])\n",
    "\n",
    "start_time_cso_lstm = time.time()\n",
    "history_cso_lstm = model_cso_lstm.fit(X_train_selected_cso, y_train, validation_split=0.2, epochs=10, batch_size=64, callbacks=[early_stopping])\n",
    "end_time_cso_lstm = time.time()\n",
    "time_execution_cso_lstm = end_time_cso_lstm - start_time_cso_lstm\n",
    "\n",
    "# Evaluasi model\n",
    "loss_cso_lstm, accuracy_cso_lstm = model_cso_lstm.evaluate(X_test_selected_cso, y_test)\n",
    "print(f\"Test Loss: {loss_cso_lstm}\")\n",
    "print(f\"Test Accuracy: {accuracy_cso_lstm}\")\n",
    "print(f\"Time Execution: {time_execution_cso_lstm}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
